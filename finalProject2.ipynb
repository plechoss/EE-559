{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, max, random\n",
    "import math\n",
    "\n",
    "class Module(object):\n",
    "    '''Module superclass from which all layers will inherit.'''\n",
    "    def __init__(self):\n",
    "        '''Constructor of the Module class.'''\n",
    "        # attributes needed for all modules\n",
    "        self.output = Tensor() # output of module (after calling forward method)\n",
    "        self.gradInput = Tensor() # gradient with respect to input to model (result of backprop)\n",
    "        self.type = str()\n",
    "        \n",
    "    def __call__(self, *inp, **kwargs):\n",
    "        '''Makes layer callable like a function and directly returns the result of forward().'''\n",
    "        return self.forward(*inp, **kwargs)\n",
    "   \n",
    "    def forward(self, *inp, **kwargs):\n",
    "        '''should get for input, and returns, a tensor or a tuple of tensors.'''\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        '''should get as input a tensor or a tuple of tensors containing the gradient of the loss\n",
    "with respect to the module's output, accumulate the gradient wrt the parameters, and return a\n",
    "tensor or a tuple of tensors containing the gradient of the loss wrt the module's input.'''\n",
    "        return self.gradInput\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        '''Sets all the gradients to zero.'''\n",
    "        self.gradInput *= 0.\n",
    "        if hasattr(self, 'weights'):\n",
    "            self.gradWeights *= 0.\n",
    "        if hasattr(self, 'biases'):\n",
    "            self.gradBiases *= 0.\n",
    "        \n",
    "    def param(self):\n",
    "        '''return a list of pairs, each composed of a parameter tensor, and a gradient tensor\n",
    "of same size. This list should be empty for parameterless modules (e.g. ReLU)'''\n",
    "        if hasattr(self, 'weights') and hasattr(self, 'biases'):\n",
    "            # BIG CHANGE for this method\n",
    "            return [(self.weights, self.gradWeights), (self.biases, self.gradBiases)]\n",
    "        elif hasattr(self, 'weights'):\n",
    "            return [(self.weights, self.gradWeights)]\n",
    "        elif hasattr(self, 'biases'):\n",
    "            return [(self.biases, self.gradBiases)]\n",
    "        else : return []\n",
    "\n",
    "class Linear(Module):\n",
    "    '''Module to implement linear layers of arbitrary size.'''\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        self.inp = Tensor()\n",
    "        self.weights = Tensor(out_dim,in_dim).normal_() # BIG CHANGE\n",
    "        self.biases = Tensor(out_dim,1).normal_()\n",
    "        self.gradWeights = Tensor(self.weights.size()).zero_()\n",
    "        self.gradBiases = Tensor(self.biases.size()).zero_()\n",
    "        self.type = 'Linear'\n",
    "           \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "        self.output = (self.weights.mm(inp.t()) + self.biases).t()\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, gradOutput):\n",
    "        self.gradInput =  (self.weights.t().mm(gradOutput.t())).t()\n",
    "        gradWeights = Tensor.mm(gradOutput.t(), self.inp)\n",
    "        gradBiases = gradOutput.t()\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.gradWeights += gradWeights\n",
    "        self.gradBiases += gradBiases.sum(1).unsqueeze(1) # unsqueeze returns vector\n",
    "        return self.gradInput\n",
    "        \n",
    "class Tanh(Module):\n",
    "    '''Module to implement Tanh acivation layer.'''\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.type = 'Tanh'\n",
    "        self.inp = Tensor()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "        self.output = inp.tanh()\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, gradOutput):\n",
    "        dtanh = (1 - self.inp.tanh().pow(2)) # self.output does not work instead\n",
    "        return gradOutput.mul(dtanh)\n",
    "    \n",
    "class ReLU(Module):\n",
    "    '''Rectified linear unit module.'''\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.inp = Tensor()\n",
    "        self.type = 'ReLU'\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp.clone()\n",
    "        inp[inp < 0] = 0\n",
    "        self.output = inp.clone()\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, gradOutput):\n",
    "        step = self.inp.clone()\n",
    "        # derivative of ReLU is step function applied to original input\n",
    "        step[step > 0] = 1\n",
    "        step[step < 0] = 0\n",
    "        self.gradInput = gradOutput.mul(step)\n",
    "        return self.gradInput\n",
    "    \n",
    "class Sequential(Module):    \n",
    "    '''Container to store several layers sequentially.'''\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "        self.size = 0\n",
    "        self.type = 'Sequential container'\n",
    "        for arg in args:\n",
    "            self.add(arg)\n",
    "        print(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'New neural net\\n'\n",
    "        for ind, module in enumerate(self.modules):\n",
    "            if module.type == 'Linear':\n",
    "                string += '   Layer ' + str(ind) + ': ' + module.type +', ' + str(module.weights.shape) + '\\n'\n",
    "            else:\n",
    "                string += '   Layer ' + str(ind) + ': ' + module.type + '\\n'\n",
    "        return string\n",
    "    \n",
    "    def add(self, module, index=None):\n",
    "        '''Add new layer at position index. By default is added as new last layer.'''\n",
    "        if index == None: index = self.size\n",
    "        if index < 0 or index > self.size:\n",
    "            raise ValueError('Supplied index is out of range for number of modules in this sequence.')\n",
    "        self.modules.insert(index, module)\n",
    "        self.size += 1\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        temp = inp.clone()\n",
    "        for module in self.modules:\n",
    "            temp = module(temp) # feed forward loop\n",
    "        return temp # BIG CHANGE dont save to self.output\n",
    "        \n",
    "    def backward(self, gradOutput):\n",
    "        temp = gradOutput.clone()\n",
    "        for module in reversed(self.modules):\n",
    "            temp = module.backward(temp)\n",
    "        return temp # dont save to self.gradInput\n",
    "\n",
    "    def param(self): # BIG CHANGE needed for the good results\n",
    "        '''returns a flattened list of each module's parameters with a tuple of the\n",
    "        actual parameter and its gradient'''\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        '''Set the gradient of each parameter in all the modules to zero.'''\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()\n",
    "            \n",
    "class LossMSE(Module):\n",
    "    '''Module to implement mean square loss.'''\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        self.inp = Tensor()\n",
    "        self.type = 'MSE loss'\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        self.inp = inp.clone()\n",
    "        self.output = (inp - targets).pow(2).sum()\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, targets):\n",
    "        self.gradInput = 2. * (self.inp - targets)\n",
    "        return self.gradInput\n",
    "    \n",
    "class LossCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super(LossCrossEntropy, self).__init__()\n",
    "        self.inp = Tensor()\n",
    "        self.type = 'Cross-entropy loss'\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, targets):\n",
    "        pass\n",
    "    \n",
    "class optimizer(object):\n",
    "    '''Class to optimize model parameters using stochastic gradient descent.'''\n",
    "    def __init__(self, parameters, eta):\n",
    "        self.params = parameters # model parameters\n",
    "        self.eta = eta # learning rate\n",
    "        \n",
    "    def step(self):\n",
    "        '''Performs one step of stochastic gradient descent.'''\n",
    "        for (param, gradParam) in self.params:\n",
    "            param -= self.eta*gradParam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    '''Generates training set uniformely distributed in [0,1], with label 1 inside\n",
    "    disk centered at [0.5, 0.5] of radius 1/sqrt(2pi) and label 0 outside.'''\n",
    "    inp = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = inp.sub(0.5).pow(2).sum(1).sub(1./(2*math.pi)).sign().sub(1).div(-2).long()\n",
    "    return inp, target\n",
    "\n",
    "def convert_to_one_hot_labels(inp, target):\n",
    "    '''Convert label vector to one hot label matrix.'''\n",
    "    tmp = inp.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp\n",
    "\n",
    "def compute_stats(model, data_input, data_target, mini_batch_size=100):\n",
    "    '''Compute accuracy and the number of wrongly predicted instances from test data.accuracy and '''\n",
    "    nb_data_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))#.reshape((mini_batch_size, 2))\n",
    "        _, predicted_classes = max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "    accuracy = 100 - (100*(nb_data_errors / len(data_target)))\n",
    "    print('Accuracy : ', accuracy, '%')\n",
    "    print('Error rate: ', 100*nb_data_errors/len(data_target), '%')\n",
    "    return\n",
    "\n",
    "\n",
    "def train_model(model, train_input, train_one_hot_target, criterion, optimizer, nb_epochs=100, mini_batch_size=5, verbose=False):\n",
    "    '''Train the given model using the give loss function with the given optimizer.'''\n",
    "    for e in range(0, nb_epochs):\n",
    "        loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            minibatch_loss = criterion.forward(output, train_one_hot_target.narrow(0, b, mini_batch_size))\n",
    "            loss += minibatch_loss\n",
    "            model.zero_grad() # zero all parameter gradients\n",
    "            model.backward(criterion.backward(train_one_hot_target.narrow(0, b, mini_batch_size)))\n",
    "            optimizer.step() # train with one step of stochastic gradient descent\n",
    "        if verbose:\n",
    "            if e%10 == 0:\n",
    "                print('Mean loss epoch {} : {:.2f} %'.format(e, 100*loss.item()/train_input.shape[0]))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.manual_seed(20)\n",
    "\n",
    "train_inp, train_target = generate_disc_set(1000)\n",
    "test_inp, test_target = generate_disc_set(100)\n",
    "\n",
    "# make variance = 1, mean = 0\n",
    "train_inp = train_inp.sub(train_inp.mean()).div(train_inp.std())\n",
    "test_inp = test_inp.sub(test_inp.mean()).div(test_inp.std())\n",
    "train_target_hot = convert_to_one_hot_labels(train_inp, train_target)\n",
    "criterion = LossMSE()\n",
    "\n",
    "model = Sequential(Linear(2, 25), Tanh(), Linear(25, 25), Tanh(), Linear(25, 2))\n",
    "optim = optimizer(model.param(), 0.001)\n",
    "\n",
    "train_model(model, train_inp, train_target_hot, criterion, optim, mini_batch_size=100, nb_epochs=100, verbose=True)\n",
    "print('\\nTraining set')\n",
    "compute_stats(model, train_inp, train_target, mini_batch_size=1)\n",
    "print('\\nTest set')\n",
    "compute_stats(model, test_inp, test_target, mini_batch_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
