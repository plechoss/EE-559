{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 : Mini deep learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to design a mini \"deep learning framework\" using only pytorch's\n",
    "tensor operations and the standard math library, hence in particular without using autograd or the\n",
    "neural-network modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework should use no pre-existing neural-network python toolbox and only work with basic pytroch operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch # but don't use nn\n",
    "\n",
    "# autograd globally off\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Constructor of the Module class.'''\n",
    "        # attributes needed for all modules\n",
    "        self.output = torch.Tensor() # output of module (after calling forward method)\n",
    "        self.gradInput = torch.Tensor() # gradient with respect to input to model (result of backprop)\n",
    "        self.type = str()\n",
    "        \n",
    "    def __call__(self, *inp, **kwargs):\n",
    "        '''Makes layer callable like a function and directly returns the result of forward().'''\n",
    "        return self.forward(*inp, **kwargs)\n",
    "   \n",
    "    def forward(self, *inp, **kwargs):\n",
    "        '''should get for input, and returns, a tensor or a tuple of tensors.'''\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        '''should get as input a tensor or a tuple of tensors containing the gradient of the loss\n",
    "with respect to the module's output, accumulate the gradient wrt the parameters, and return a\n",
    "tensor or a tuple of tensors containing the gradient of the loss wrt the module's input.'''\n",
    "        return self.gradInput\n",
    "    \n",
    "    def zeroGrad(self):\n",
    "        '''Sets all the gradients to zero.'''\n",
    "        self.gradInput *= 0.\n",
    "        if hasattr(self, 'weights'):\n",
    "            self.gradWeights *= 0.\n",
    "        if hasattr(self, 'biases'):\n",
    "            self.gradBiases *= 0.\n",
    "        \n",
    "    def param(self):\n",
    "        '''return a list of pairs, each composed of a parameter tensor, and a gradient tensor\n",
    "of same size. This list should be empty for parameterless modules (e.g. ReLU)'''\n",
    "        if hasattr(self, 'weights') and hasattr(self, 'biases'):\n",
    "            return {self.weights, self.biases}, {self.gradWeights, self.gradBiases}\n",
    "        elif hasattr(self, 'weights'):\n",
    "            return self.weights, self.gradWeights\n",
    "        elif hasattr(self, 'biases'):\n",
    "            return self.biases, self.gradBiases\n",
    "        else : return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should implement at least the modules Linear (fully connected layer), ReLU, Tanh, Sequential\n",
    "to combine several modules in basic sequential structure, and LossMSE to compute the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    '''Fully connected linear layer.'''\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.weights = torch.Tensor(size=(out_features, in_features))     \n",
    "        self.gradWeights = torch.empty(size=(out_features, in_features))\n",
    "        if bias:\n",
    "            self.biases = torch.empty(size=(out_features, 1))\n",
    "            self.gradBiases = torch.empty(size=(in_features, out_features))\n",
    "        self.inp = torch.Tensor() # to keep track of past input values\n",
    "        self.type = 'Linear'\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        self.output = torch.mm(self.weights, inp)\n",
    "        if hasattr(self, 'biases'):\n",
    "            self.output.add_(self.biases)\n",
    "        self.inp = inp\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradOutput): # feed it input and loss\n",
    "        self.gradInput = torch.mm(self.weights.t(), gradOutput)\n",
    "        self.gradWeights = torch.mm(gradOutput, self.inp.t())\n",
    "        if hasattr(self, 'biases'):\n",
    "            self.gradBiases = gradOutput\n",
    "        return self.gradInput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lossMSE(Module):\n",
    "    '''Compute MSE loss.'''\n",
    "    \n",
    "    # can this be omitted since it's unchanged?\n",
    "    def __init__(self):\n",
    "        super(lossMSE, self).__init__()\n",
    "        self.inp = torch.Tensor() # to keep track of past input values\n",
    "        self.type = 'MSE loss'\n",
    "        \n",
    "    def forward(self, inp, target):\n",
    "        '''Calculate MSE loss.'''\n",
    "        self.output = sum((inp-target).pow(2))/(2*len(inp))\n",
    "        self.inp = inp\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target):\n",
    "        self.gradInput = self.inp-target\n",
    "        return self.gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    '''Rectifed linear unit activation layer.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.type = 'ReLU'\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.output = inp.clone()\n",
    "        self.output[self.output<0] = 0 # kill all negative terms\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        self.gradInput = gradOutput.clone()\n",
    "        self.gradInput[gradOutput<0] = 0\n",
    "        self.gradInput[gradOutput>0] = 1\n",
    "        return self.gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    '''Sigmoid activation layer.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "        self.type = 'Sigmoid'\n",
    "    \n",
    "    @staticmethod  # can be used without creating a sigmoid object\n",
    "    def sigmoidFct(inp):\n",
    "        '''Sigmoid function.'''\n",
    "        return math.exp(inp)/(1. + math.exp(inp))\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.output = sigmoidFct(inp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        # apply derivative of sigmoid\n",
    "        self.gradInput = sigmoidFct(gradOutput)*(1.-sigmoidFct(gradOutput)) \n",
    "        return self.gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    '''Container to store several layers sequentially.'''\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "        self.size = 0\n",
    "        self.type = 'Sequential containter'\n",
    "        for arg in args:\n",
    "            self.add(arg)\n",
    "        print(self)\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = 'New neural net\\n'\n",
    "        for ind, module in enumerate(self.modules):\n",
    "            string += '   Layer ' + str(ind) + ': ' + module.type + '\\n'\n",
    "        return string\n",
    "    \n",
    "    def add(self, module, index=None):\n",
    "        '''Add new layer at position index. By default is added as new last layer.'''\n",
    "        if index == None: index = self.size\n",
    "        if index < 0 or index > self.size:\n",
    "            raise ValueError('Supplied index is out of range for number of modules in this sequence.')\n",
    "        self.modules.insert(index, module)\n",
    "        self.size += 1\n",
    "        # check if module was added as first or last layer\n",
    "        if index == self.size:\n",
    "            self.output = self.modules[-1].output\n",
    "        if index == 0:\n",
    "            self.gradInput = self.modules[0].gradInput\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        temp = inp.clone()\n",
    "        for module in self.modules:\n",
    "            temp = module(temp) # feed forward loop\n",
    "            module.output = temp\n",
    "        self.output = temp\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        temp = gradOutput.clone()\n",
    "        for module in reversed(self.modules):\n",
    "            temp = module.backward(temp) # backpropagation\n",
    "            module.gradInput = temp\n",
    "        self.gradInput = temp\n",
    "        return self.gradInput\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondSeq = Sequential(Linear(2, 1), ReLU(), Sigmoid(), lossMSE())\n",
    "print(secondSeq.size)\n",
    "print(secondSeq.modules)\n",
    "print(secondSeq.modules[0].param())\n",
    "tripSeq = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relutest = ReLU()\n",
    "inp = torch.Tensor([3., 2., 1., 0., -1.])\n",
    "print(inp)\n",
    "print(relutest(inp))\n",
    "print(relutest.backward(inp))\n",
    "print(relutest.param())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseTest = lossMSE()\n",
    "inp = torch.full((2, 1), 6.)\n",
    "target = torch.full((2, 1), 1.)\n",
    "print(mseTest(inp, target))\n",
    "print(mseTest.backward(target))\n",
    "print(mseTest.param())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseTest = lossMSE()\n",
    "\n",
    "inp = torch.full((2, 1), 2.)\n",
    "target = torch.full((5, 1), 1.)\n",
    "\n",
    "linTest = Linear(len(inp), len(target))\n",
    "\n",
    "pred = linTest(inp)\n",
    "print(pred)\n",
    "print(target)\n",
    "loss = mseTest(pred, target)\n",
    "print(loss)\n",
    "linBackward = linTest.backward(mseTest.backward(target))\n",
    "\n",
    "# todo : check this makes sense!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
